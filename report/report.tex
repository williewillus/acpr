\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}

\begin{document}

\title{Asynchronous I/O for \texttt{cp -r}}
\date{\today}
\author{Vincent Lee \\
        Gualberto A. Guzman}

\maketitle

\section{Goals}

Our project proposal is to use Linux's asynchronous IO interfaces (aio) to build
an optimized version of cp -r. Our implementation will take an existing
directory name and a new name and copy the entire directory tree to the location
specified. We aim for our program to be competitive with \texttt{cp -r}'s speeds (within 1.5x slower).

\section{Major Decisions} 

There were two major choices available to us for asynchronous file IO on Linux.
The first was the \textbf{POSIX AIO} interface, which is standardized as a set
of library calls across all POSIX-compliant environments. However, according to
the man pages, glibc simulates POSIX AIO completely in userspace~\cite{aio7}.
This means that the kernel is not able to make scheduling and reordering
decisions that may affect performance. Instead, we decided to use the
nonportable \textbf{Linux AIO} system calls.

For the implementation itself, \textbf{C++} was chosen as the programming
language. In addition to being a fast systems programming language, it also
provides useful abstractions and standard libraries, as well as access to the
Boost libraries. The Boost Filesystem library was used for its
\texttt{recursive\_directory\_iterator}, an iterator which yields all paths
along a breadth-first traversal from a given root path. For each path, if the
file is small enough, it is copied directly using a Boost call. This is because,
below a certain point, the overhead of initializing and submitting an
asynchronous IO task dominates the cost of simply using \texttt{read} and
\texttt{write}. This threshold is configurable and the effects of varying it are
explored in the Evaluation.

\section{Implementation}
After parsing all arguments, \texttt{acpr} initializes a recursive directory iterator from Boost, which yields a breadth-first traversal of every path in the given source directory. For each path, we run \texttt{lstat} to discover if it is a file or directory. If a directory, we create it immediately in the output path with \texttt{mkdir}. Since the iterator is breadth-first, we are guaranteed that all parent directories exist at the time of calling. If a file and it is below the AIO threshold, we copy it using a Boost utility function implemented with \texttt{read} and \texttt{write}. Otherwise, we use the AIO system.

\subsection{AIO Copying}
A copy of a single file using AIO is represented by a class \texttt{CopyTask}. This class encapsulates the state of each copy, as well as a fixed number of  \texttt{iocb}'s. An \texttt{iocb} is the AIO system's unit of work. Each time \texttt{CopyTask::schedule\_next\_read} is called, we scan to see if there are any free \texttt{iocb}'s, and attach to each of them offsets to read, the file descriptor to read from, a buffer to read into, and a callback to invoke when complete. Once enough \texttt{iocb}'s have been buffered, the system call \texttt{io\_submit} is invoked, passing all initialized \texttt{iocb}'s to the kernel. 

At the end of each loop of the recursive directory iterator, another system call \texttt{io\_getevents} is called to gather any completed reads and writes from the kernel and invoke their callbacks. This call takes a timeout that represents the maximum time the user is willing to wait for the kernel to give it events. Whenever a callback for a write is invoked, it invokes \texttt{CopyTask::schedule\_next\_read}. The \texttt{CopyTask} corresponding to each \texttt{iocb} is kept in a private lookup table, enabling it to be recovered from the callbacks, which are global function pointers. Whenever a callback for a read is invoked, it simply reinitializes the \texttt{iocb} into a write, changes the file descriptor to the output file, and immediately resubmits it. Here, we potentially could have performed batching of writes to reduce the number of system calls, but due to complexity concerns and time limits, decided to leave it as-is.

After the directory iterator finishes, the event queue is then drained by calling \texttt{io\_getevents} in a loop with unlimited timeout until the \texttt{iocb} to \texttt{CopyTask} lookup table is empty, meaning all \texttt{CopyTask}s have completed.

\section{Challenges and Limitations}
A function critical to our implementation,
\texttt{boost::filesystem::relativize}, was missing in the version of Boost
installed on the lab computers. This function takes two absolute paths and makes
one into a path relative to the other, so that it can be appended to the
destination directory. We backported this function from later Boost versions by
simply copying the relatively small implementation from the Boost source
control.

The userspace header library that wrapped the Linux AIO system also implemented
a particular call naively. \texttt{io\_queue\_run} is a function that polls the
kernel event queues and runs callbacks. However, we looked at the implementation
and discovered that it was receiving events one by one, making a system call for
every single one. We replaced calls to this function with direct system calls to
read the event queue ourselves, reducing the amount of system calls needed to process the same number of events.

During early testing, we attempted to copy the Linux 4.4.97 kernel tree on an SSD, a 711 megabyte repository of mostly small files. acpr used over 7 minutes before it was killed, while cp used only 1 second. Some profiling indicated that the AIO subsystem finished very quickly, it was the \texttt{fsync} call for each file that slowed everything down. Disabling the \texttt{fsync} by default sped the copy up to about 3 seconds. We verified that \texttt{cp} also chose not to call it by observing the system calls it made using \texttt{strace}. 

Currently, attributes and ownership are not completely copied from the source to the destination. In addition, due to complications arising from relative symbolic links, all symbolic links are skipped when copying.

The event queue is processed with the single calling thread. A potential optimization would have been to have a separate thread driving the event queue while the main thread iterates the directory, but for simplicity, a single-thread implementation was done.

\section{Evaluation}
acpr was evaluated on two main platforms, a laptop running Linux with an SSD, and the csres server machines. The test script first runs \texttt{cp -r} on the directories under test, then runs \texttt{acpr} to another destination directory, and times both. The test script also has the ability to sync and clear the buffer cache by calling \texttt{echo 3 > /proc/sys/vm/drop\_caches}, which we opt to do for all cases below, to test the AIO system's ability to reorder requests effectively for IO. After copying, the source and destination directories will be compared using \texttt{diff -r}, which will perform a deep diff of the entire subtree to find corrupt, absent, or extra files from the copy.

We test two major cases, copying a large file and copying the Linux 4.4.97 source tree. The first exercises acpr's ability to fully utilize the disk's speed, while the second emulates real-world scenarios of copying around subtrees filled with files of varying sizes. We note, however, that since the Linux kernel contains some relative symlinks, which we skip, the test script's error-checking will report differences. We have verified that manually using \texttt{diff -r} that acpr is copying the output correctly. 

\subsection{Thinkpad X1 Carbon}
The laptop is a 2013 Thinkpad X1 Carbon, with Arch Linux, updated to the latest packages as of December 5, 2017. The laptop has an Intel i7-4600U processor clocked at 2.1 GHz, 8 GB of RAM, and 4 GB of swap space configured. The tests were run on the owner's home directory, which is on a local ext4 partition on a Samsung MZNTD256 SSD.

\subsubsection{Copying a Single Large File}
First, we compare a run of acpr with all default options with cp -r. cp -r copies the ISO in about 3.5 seconds on average, coming to an average of 251 MB/s throughput. acpr is weaker, copying the ISO in about 9 seconds, coming to an average of 96.9 MB/s throughput. However, if acpr calls \texttt{fallocate} to preallocate the destination file and \texttt{readahead} on the source file, then it can copy the ISO in 3.75 seconds, a throughput of 234.6 MB/s. This is likely because the AIO system is more indirect, and the system is perhaps unable to detect that a sequential read is occurring, unlike \texttt{cp} where the repeated calls to \texttt{read} and \texttt{write} make it very clear that a sequential copy is occurring.

Next, we run the copy with larger block sizes. By default, acpr copies 64K per AIO operation. Increasing the size may not make much difference if the files are smaller than 64K anyway, but should decrease the number of system calls and improve locality of the buffers when copying a single large file. When increasing the block size to 128K, runtime decreases from the baseline of 9 seconds to 5.64 seconds and a throughput of 155.9 MB/s. Further increasing the block size to 256K again improves the performance to 3.55s and 247.6 MB/s. However, increasing again to 512K yields a performance drop to 4.35s and 201.9 MB/s. This probably due to overhead allocating and copying such large buffers around.

Finally, we test what happens if we vary the timeout when gathering events. By default, \texttt{acpr} allows the kernel 5 ms to respond with events, yielding the baseline measurement of 9 seconds and 96.9 MB/s. Increasing this timeout to 10 ms improves performance to 3.81 seconds and 230.9 MB/s, while decreasing it to 1 ms also improves performance in a similar manner to 3.23 seconds and 272.4 MB/s. Eliminating the timeout (passing 0 to the kernel) also gives a similar boost to 3.26s and 269.2 MB/s. It is not clear from these data if varying the timeout had much of a direct effect on acpr's speed, since there is no clear pattern of change. A possible source of error could be background system noise, though only the terminal emulator and \LaTeX editor were running during the experiment.

\subsubsection{Copying Linux Source Tree}
First, we compare a run of \texttt{acpr} with all default options vs. \texttt{cp -r}. \texttt{cp -r} copies the 711 MB source tree in 12.75s for 46.48 MB/s throughput, while \texttt{acpr} manages 15.05s for 39.3 MB/s throughput. Note how much lower this is than copying a single file, since there is much more filesystem overhead to open, stat, and close each individual file as opposed to just one. 

The Linux source tree is composed of many different sizes of file. The default threshold for \texttt{acpr} to use AIO is 1K, which maybe have too high of an overhead. We instead tried raising the threshold for using AIO to 4K, which would still copy 25984 files in the source tree using AIO. Doing this sped up execution to 13.69s and 43.2 MB/s. Further raising the threshold to 8K does not give a similar boost, remaining at 13.69s and 43.3 MB/s.

The second experiment was to use a \textit{smaller} block size. Since many of the files are source files that are a handful of KB large, the default block size of 64K may have resulted in unnecessary overhead allocating space that is ends up going unused. Reducing the block size to 32K yields a small speedup to 14.37s and 41.22 MB/s; reducing further to 16K does not produce further improvements, staying at 14.37s and 41.25 MB/s.

The third experiment was to vary the number of \texttt{iocb}'s used in copying each file. The default is 5, and \texttt{acpr} waits until it gathers 5 initialized \texttt{iocb}'s before it submits them to the kernel. So theoretically, raising the number used should reduce the number of system calls made, as they are batched together more. However, we saw reduced performance - for 16 \texttt{iocb}'s per file, we got 17.54s and 33.79 MB/s; for 32, we got 17.23s and 34.4 MB/s.

Our final experiment was to vary the timeout when gathering events. Changing the default of 5 ms to 10 ms gives a negligible performance boost to 14.52s and 40.79 MB/s; lowering it to 1 ms does the same: 14.13s and 41.949 MB/s, as does eliminating the timeout completely: 14.46s and 40.97 MB/s. Combined with the results from copying the Lubuntu ISO, it seems that in the cases we have set up, the timeout passed to the kernel when retrieving events doesn't matter, meaning that the kernel is almost always ready with events for us to process. This potentially could matter if we had extended \texttt{acpr} to use multiple threads for event processing.

\subsection{csres}
The csres server machines run Ubuntu LTS 16.04.3, and have 32 Intel Xeon E5-2620 v4 processors clocked at 2.1 GHz, 126 GB of RAM, and 128 GB of swap space configured. The tests were run on the /var/tmp directory, which is on a local ext4 partition, instead of the NFS home directories. The partition is on a Seagate 7200RPM enterprise grade hard disk.

\subsubsection{Copying a Single Large File}
The same test cases as above were performed on \texttt{zerberus.csres.utexas.edu}. Overall throughput was lower, as expected, with the default configuration taking 5.86s and 150.18 MB/s. \texttt{cp -r} barely did better at 5.82s and 151.15 MB/s.

Employing fallocate and readahead actually slowed performance down to 5.93s and 148.40 MB/s, though the difference is well within deviation. For the rest of the experiments (adjusting the block size and timeout wait), the results remained very close to the default configuration (the detailed data can be seen in the figures and on the repository). We attribute this to a bottleneck that is external to both \texttt{cp} and \texttt{acpr} - disk latency. 

To test this, we ran the baseline and fallocate cases again, without the the flag that calls \texttt{sync} and \texttt{echo 3 > /proc/sys/vm/drop\_caches} every iteration. As expected, throughput was much higher as our calls were now being serviced straight from memory. What was more surprising, however, was that \texttt{acpr} had a clear speedup over \texttt{cp -r} in both cases. In the default configuration with no syncing, \texttt{cp -r} manages 1.49s and 589.59 MB/s while \texttt{acpr} finishes in 0.9s and 976 MB/s. With fallocate and readahead, \texttt{cp -r} finishes in 0.85s and 1033.97 MB/s and \texttt{acpr} in 0.82s and 1068 MB/s.

\subsubsection{Copying Linux Source Tree}
Both \texttt{cp -r} and \texttt{acpr} performed quite poorly copying the Linux 4.4.97 kernel source tree, with \texttt{cp -r} taking 14.19s and 41.77 MB/s, and \texttt{acpr} taking 34.6s and 17.11 MB/s. While increasing the threshold of using AIO did improve performance, the change was miniscule. Likewise, for all other cases (adjusting block size, adjusting \texttt{iocb} count, and adjusting timeout), there was no significant change from the default configuration. Again, we attribute this to factors beyond both programs' control. \texttt{acpr} is probably slower because the kernel contains many small files, so the overhead of setting up AIO, allocating buffers, etc. dominates the time to actually transfer the data.

\section{Conclusion}
We have produced a prototype of a \texttt{cp -r} clone that uses Linux's Asynchronous IO interface to perform its data transfers. While it does not outperform \texttt{cp -r} except for two cases, it remains within acceptable performance bounds, especially on SSD hardware. Further improvements would include a more robust event handling system with multiple threads, support for symbolic links, support for attribute copying, and batching of write task submissions.

\section{Time Spent}

About 1.5 hours were spent researching the state of asynchronous IO, finding
documentation for the chosen libraries, and setting up the build system of the
project.
About 8 hours were spent overall creating the acpr program itself, including planning, programming, and optimization.
About 5 hours were spent creating test cases, executing test cases, and collecting the data.
About 2 hours were spent creating the report and presentation.

\begin{thebibliography}{99}
        \bibitem{aio7}
        aio (7), http://man7.org/linux/man-pages/man7/aio.7.html
\end{thebibliography}

\end{document}
